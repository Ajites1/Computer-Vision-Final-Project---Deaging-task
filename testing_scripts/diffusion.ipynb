{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4d8317-6f07-4480-9c2b-b5da5d6424ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8ee8b8-d9f8-417a-81ed-ed578c75e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 1. AGE & IDENTITY ENCODERS\n",
    "# =======================\n",
    "\n",
    "class AgeEncoder(nn.Module):\n",
    "    \"\"\"Encodes age into an embedding vector\"\"\"\n",
    "    def __init__(self, max_age=100, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.age_embedding = nn.Embedding(max_age + 1, embed_dim)\n",
    "        \n",
    "    def forward(self, age):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            age: tensor of shape (batch,) with integer ages\n",
    "        Returns:\n",
    "            age_embed: (batch, embed_dim)\n",
    "        \"\"\"\n",
    "        return self.age_embedding(age)\n",
    "\n",
    "\n",
    "class IdentityEncoder(nn.Module):\n",
    "    \"\"\"Extracts identity features from input image\"\"\"\n",
    "    def __init__(self, output_dim=256):\n",
    "        super().__init__()\n",
    "        # Simple CNN for identity extraction\n",
    "        # In practice, you might use a pretrained face recognition model\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1),  # 128x128 -> 64x64\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # 64x64 -> 32x32\n",
    "            nn.GroupNorm(8, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),  # 32x32 -> 16x16\n",
    "            nn.GroupNorm(8, 256),\n",
    "            nn.SiLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input image (batch, 3, H, W)\n",
    "        Returns:\n",
    "            identity_embed: (batch, output_dim)\n",
    "        \"\"\"\n",
    "        return self.encoder(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527b1d14-ae3b-4c0b-bbee-bc55cda6f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 2. U-NET WITH CONDITIONING\n",
    "# =======================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with time, age, and identity conditioning\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, time_dim=256, cond_dim=512):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        \n",
    "        # Conditioning projections\n",
    "        self.time_mlp = nn.Linear(time_dim, out_ch)\n",
    "        self.cond_mlp = nn.Linear(cond_dim, out_ch)\n",
    "        \n",
    "        self.shortcut = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, t_embed, cond_embed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, in_ch, H, W)\n",
    "            t_embed: time embedding (batch, time_dim)\n",
    "            cond_embed: concatenated age+identity embedding (batch, cond_dim)\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "        \n",
    "        # Add conditioning\n",
    "        h = h + self.time_mlp(t_embed)[:, :, None, None]\n",
    "        h = h + self.cond_mlp(cond_embed)[:, :, None, None]\n",
    "        \n",
    "        h = F.silu(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        \n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    \"\"\"Sinusoidal time embedding\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class ConditionalUNet(nn.Module):\n",
    "    \"\"\"U-Net conditioned on time, age, and identity\"\"\"\n",
    "    def __init__(self, img_channels=3, base_ch=64, time_dim=256, age_dim=128, identity_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        cond_dim = age_dim + identity_dim  # Combined conditioning\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_dim),\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim * 4, time_dim)\n",
    "        )\n",
    "        \n",
    "        # Conditioning projection\n",
    "        self.cond_proj = nn.Sequential(\n",
    "            nn.Linear(cond_dim, cond_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(cond_dim * 2, cond_dim)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ResidualBlock(img_channels, base_ch, time_dim, cond_dim)\n",
    "        self.enc2 = ResidualBlock(base_ch, base_ch * 2, time_dim, cond_dim)\n",
    "        self.enc3 = ResidualBlock(base_ch * 2, base_ch * 4, time_dim, cond_dim)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(base_ch * 4, base_ch * 4, time_dim, cond_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.dec3 = ResidualBlock(base_ch * 8, base_ch * 2, time_dim, cond_dim)\n",
    "        self.dec2 = ResidualBlock(base_ch * 4, base_ch, time_dim, cond_dim)\n",
    "        self.dec1 = ResidualBlock(base_ch * 2, base_ch, time_dim, cond_dim)\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv2d(base_ch, img_channels, 1)\n",
    "        \n",
    "    def forward(self, x, t, age_embed, identity_embed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: noisy image (batch, 3, H, W)\n",
    "            t: timestep (batch,)\n",
    "            age_embed: (batch, age_dim)\n",
    "            identity_embed: (batch, identity_dim)\n",
    "        Returns:\n",
    "            predicted noise (batch, 3, H, W)\n",
    "        \"\"\"\n",
    "        # Prepare conditioning\n",
    "        t_embed = self.time_mlp(t)\n",
    "        cond_embed = torch.cat([age_embed, identity_embed], dim=1)\n",
    "        cond_embed = self.cond_proj(cond_embed)\n",
    "        \n",
    "        # Encoder\n",
    "        e1 = self.enc1(x, t_embed, cond_embed)\n",
    "        e2 = self.enc2(self.pool(e1), t_embed, cond_embed)\n",
    "        e3 = self.enc3(self.pool(e2), t_embed, cond_embed)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool(e3), t_embed, cond_embed)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d3 = self.dec3(torch.cat([self.up(b), e3], dim=1), t_embed, cond_embed)\n",
    "        d2 = self.dec2(torch.cat([self.up(d3), e2], dim=1), t_embed, cond_embed)\n",
    "        d1 = self.dec1(torch.cat([self.up(d2), e1], dim=1), t_embed, cond_embed)\n",
    "        \n",
    "        return self.out(d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ecde62-5473-4d05-8e7f-9124f97a605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 3. COMPLETE MODEL\n",
    "# =======================\n",
    "\n",
    "class AgeTransformationDiffusion(nn.Module):\n",
    "    \"\"\"Complete age transformation diffusion model\"\"\"\n",
    "    def __init__(self, max_age=100, img_size=256):\n",
    "        super().__init__()\n",
    "        self.age_encoder = AgeEncoder(max_age=max_age, embed_dim=128)\n",
    "        self.identity_encoder = IdentityEncoder(output_dim=256)\n",
    "        self.unet = ConditionalUNet(\n",
    "            img_channels=3,\n",
    "            base_ch=64,\n",
    "            time_dim=256,\n",
    "            age_dim=128,\n",
    "            identity_dim=256\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_noisy, t, current_age, target_age, x_clean):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_noisy: noisy image at timestep t (batch, 3, H, W)\n",
    "            t: diffusion timestep (batch,)\n",
    "            current_age: current age of person (batch,)\n",
    "            target_age: target age for transformation (batch,)\n",
    "            x_clean: clean reference image for identity (batch, 3, H, W)\n",
    "        Returns:\n",
    "            predicted_noise: (batch, 3, H, W)\n",
    "        \"\"\"\n",
    "        # Extract embeddings\n",
    "        identity_embed = self.identity_encoder(x_clean)\n",
    "        target_age_embed = self.age_encoder(target_age)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise_pred = self.unet(x_noisy, t, target_age_embed, identity_embed)\n",
    "        return noise_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee3f448-0abd-4920-a335-01309b6469eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 4. DATASET\n",
    "# =======================\n",
    "\n",
    "class AgeTransformationDataset(Dataset):\n",
    "    \"\"\"Dataset for age transformation training\"\"\"\n",
    "    def __init__(self, image_paths, ages, identities=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths: list of image file paths\n",
    "            ages: list of ages corresponding to each image\n",
    "            identities: optional list of identity IDs (for longitudinal data)\n",
    "            transform: torchvision transforms\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.ages = ages\n",
    "        self.identities = identities\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((256, 256), Image.LANCZOS),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(self.image_paths[idx])\n",
    "\n",
    "        if image.dtype == np.float32 or image.dtype == np.float64:\n",
    "            # If normalized to [0, 1], scale to [0, 255]\n",
    "            if image.max() <= 1.0:\n",
    "                image = (image * 255).astype(np.uint8)\n",
    "            else:\n",
    "                image = image.astype(np.uint8)\n",
    "        elif image.dtype != np.uint8:\n",
    "            image = image.astype(np.uint8)\n",
    "        \n",
    "        # Handle channel order: (H, W, C) or (C, H, W)\n",
    "        if image.ndim == 3:\n",
    "            if image.shape[0] == 3 or image.shape[0] == 1:\n",
    "                # (C, H, W) -> (H, W, C)\n",
    "                image = np.transpose(image, (1, 2, 0))\n",
    "        # Convert to PIL Image\n",
    "        if image.shape[-1] == 1:\n",
    "            image = Image.fromarray(image.squeeze(), mode='L').convert('RGB')\n",
    "        else:\n",
    "            image = Image.fromarray(image, mode='RGB')\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        age = torch.tensor(self.ages[idx], dtype=torch.long)\n",
    "        \n",
    "        item = {'image': image, 'age': age}\n",
    "        if self.identities is not None:\n",
    "            item['identity'] = self.identities[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LongitudinalPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for longitudinal pairs (same person at different ages).\n",
    "    Each item returns two images of the same person.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs_data, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pairs_data: List of dicts with format:\n",
    "                [\n",
    "                    {\n",
    "                        'img1_path': 'person1_age20.jpg',\n",
    "                        'age1': 20,\n",
    "                        'img2_path': 'person1_age50.jpg', \n",
    "                        'age2': 50,\n",
    "                        'identity': 'person1'\n",
    "                    },\n",
    "                    ...\n",
    "                ]\n",
    "            transform: torchvision transforms\n",
    "        \"\"\"\n",
    "        self.pairs_data = pairs_data\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs_data[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.load(pair['img1_path'])\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"np.load failed for {pair['img1_path']}: {e}\")\n",
    "\n",
    "        if image.dtype == np.float32 or image.dtype == np.float64:\n",
    "            # If normalized to [0, 1], scale to [0, 255]\n",
    "            if image.max() <= 1.0:\n",
    "                image = (image * 255).astype(np.uint8)\n",
    "            else:\n",
    "                image = image.astype(np.uint8)\n",
    "        elif image.dtype != np.uint8:\n",
    "            image = image.astype(np.uint8)\n",
    "        \n",
    "        # Handle channel order and shape\n",
    "        if image.ndim == 3:\n",
    "            if image.shape[0] in [1, 3]:  # (C, H, W) format\n",
    "                image = np.transpose(image, (1, 2, 0))\n",
    "        elif image.ndim == 2:  # Grayscale (H, W)\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "        \n",
    "        # Ensure contiguous array - CRITICAL for PIL\n",
    "        image = np.ascontiguousarray(image)\n",
    "        # Convert to PIL Image\n",
    "        if image.shape[-1] == 1:\n",
    "            image = Image.fromarray(image.squeeze(), mode='L').convert('RGB')\n",
    "        else:\n",
    "            image = Image.fromarray(image, mode='RGB')\n",
    "        img1 = self.transform(image)\n",
    "        \n",
    "        try:\n",
    "            image = np.load(pair['img2_path'])\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"np.load failed for {pair['img1_path']}: {e}\")\n",
    "        \n",
    "        if image.dtype == np.float32 or image.dtype == np.float64:\n",
    "            # If normalized to [0, 1], scale to [0, 255]\n",
    "            if image.max() <= 1.0:\n",
    "                image = (image * 255).astype(np.uint8)\n",
    "            else:\n",
    "                image = image.astype(np.uint8)\n",
    "        elif image.dtype != np.uint8:\n",
    "            image = image.astype(np.uint8)\n",
    "        \n",
    "        # Handle channel order: (H, W, C) or (C, H, W)\n",
    "        if image.ndim == 3:\n",
    "            if image.shape[0] == 3 or image.shape[0] == 1:\n",
    "                # (C, H, W) -> (H, W, C)\n",
    "                image = np.transpose(image, (1, 2, 0))\n",
    "        elif image.ndim == 2:  # Grayscale (H, W)\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "        \n",
    "        # Ensure contiguous array - CRITICAL for PIL\n",
    "        image = np.ascontiguousarray(image)\n",
    "        # Convert to PIL Image\n",
    "        if image.shape[-1] == 1:\n",
    "            image = Image.fromarray(image.squeeze(), mode='L').convert('RGB')\n",
    "        else:\n",
    "            image = Image.fromarray(image, mode='RGB')\n",
    "        img2 = self.transform(image)\n",
    "        return {\n",
    "            'img1': img1,\n",
    "            'age1': torch.tensor(pair['age1'], dtype=torch.long),\n",
    "            'img2': img2,\n",
    "            'age2': torch.tensor(pair['age2'], dtype=torch.long),\n",
    "            'identity': torch.tensor(pair['identity'], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71e343a6-b146-4f8c-b2cb-f27f452682e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 5. TRAINING UTILITIES\n",
    "# =======================\n",
    "\n",
    "class DiffusionTrainer:\n",
    "    \"\"\"Handles diffusion training with both datasets\"\"\"\n",
    "    def __init__(self, model, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.model = model\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        # Linear beta schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        self.betas = self.betas.to(device)\n",
    "        self.alphas = self.alphas.to(device)\n",
    "        self.alphas_cumprod = self.alphas_cumprod.to(device)\n",
    "    def add_noise(self, x0, t):\n",
    "        \"\"\"Add noise to clean images according to timestep t\"\"\"\n",
    "        sqrt_alpha_cumprod = torch.sqrt(self.alphas_cumprod[t])\n",
    "        sqrt_one_minus_alpha_cumprod = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "        \n",
    "        noise = torch.randn_like(x0)\n",
    "        x_t = sqrt_alpha_cumprod[:, None, None, None] * x0 + \\\n",
    "              sqrt_one_minus_alpha_cumprod[:, None, None, None] * noise\n",
    "        return x_t, noise\n",
    "    \n",
    "    def training_step_cross_sectional(self, batch, device):\n",
    "        \"\"\"Training step for cross-sectional dataset\"\"\"\n",
    "        images = batch['image'].to(device)\n",
    "        ages = batch['age'].to(device)\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, self.num_timesteps, (batch_size,), device=device)\n",
    "        \n",
    "        # Add noise\n",
    "        x_noisy, noise_true = self.add_noise(images, t)\n",
    "        \n",
    "        # For cross-sectional: use same image for identity and target\n",
    "        noise_pred = self.model(x_noisy, t, ages, ages, images)\n",
    "        \n",
    "        # MSE loss\n",
    "        loss = F.mse_loss(noise_pred, noise_true)\n",
    "        return loss\n",
    "    \n",
    "    def training_step_longitudinal(self, img1, age1, img2, age2, device):\n",
    "        \"\"\"\n",
    "        Training step for longitudinal dataset (same person, different ages)\n",
    "        This helps learn identity preservation\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = img1.shape[0]\n",
    "        t = torch.randint(0, self.num_timesteps, (batch_size,), device=device)\n",
    "        \n",
    "        # Add noise to target age image\n",
    "        x_noisy, noise_true = self.add_noise(img2, t)\n",
    "        \n",
    "        # Use img1 for identity, age2 for target age\n",
    "        noise_pred = self.model(x_noisy, t, age1, age2, img1)\n",
    "        \n",
    "        loss = F.mse_loss(noise_pred, noise_true)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d16a9ab-850f-494e-8555-13d15baef1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_sectional(path):\n",
    "    with open(path, 'r') as f:\n",
    "        loaded_dict = json.load(f)\n",
    "    imgs,ages = [],[]\n",
    "    for i in loaded_dict:\n",
    "        for j in loaded_dict[i]:\n",
    "            ages.append(int(i))\n",
    "            imgs.append(loaded_dict[i][j])\n",
    "    return imgs,ages\n",
    "\n",
    "\n",
    "def get_longitudinal_sectional(path):\n",
    "    with open(path, 'r') as f:\n",
    "        loaded_dict = json.load(f)\n",
    "    all_pairs = []\n",
    "    person = 0\n",
    "    for i in loaded_dict:\n",
    "        temp_ages = []\n",
    "        temp_imgs = []\n",
    "        for j in loaded_dict[i]:\n",
    "            temp_ages.append(int(j.split(\"_\")[0]))\n",
    "            temp_imgs.append(loaded_dict[i][j])\n",
    "        x = 0\n",
    "        n = len(temp_ages)\n",
    "        while(x<n-1):\n",
    "            d = {}\n",
    "            d['img1_path'] = temp_imgs[x]\n",
    "            d['age1'] = temp_ages[x]\n",
    "            d['img2_path'] = temp_imgs[x+1]\n",
    "            d['age2'] = temp_ages[x+1]\n",
    "            d['identity'] = person\n",
    "            x+=1\n",
    "            all_pairs.append(d)\n",
    "        person+=1\n",
    "    return all_pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24503e0d-2213-4041-b7d4-375ea07a7ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 5,680,131 parameters\n",
      "Device: cuda\n",
      "\n",
      "Model architecture ready for training!\n",
      "Epoch 0, Cross-sectional batch 0, Loss: 1.1190\n",
      "Epoch 0, Cross-sectional batch 100, Loss: 0.0915\n",
      "Epoch 0, Cross-sectional batch 200, Loss: 0.0140\n",
      "Epoch 0, Cross-sectional batch 300, Loss: 0.0376\n",
      "Epoch 0, Cross-sectional batch 400, Loss: 0.0325\n",
      "Epoch 0, Cross-sectional batch 500, Loss: 0.0260\n",
      "Epoch 0, Cross-sectional batch 600, Loss: 0.0378\n",
      "Epoch 0, Cross-sectional batch 700, Loss: 0.0096\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "model = AgeTransformationDiffusion(max_age=120).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "trainer = DiffusionTrainer(model)\n",
    "\n",
    "# Example: Create datasets\n",
    "# Cross-sectional dataset\n",
    "cross_sectional_paths, cross_sectional_ages = get_cross_sectional(\"/projects/standard/csci5561/shared/G8/data/qtk.json\")\n",
    "\n",
    "cross_dataset = AgeTransformationDataset(cross_sectional_paths, cross_sectional_ages)\n",
    "\n",
    "cross_dataloader = DataLoader(\n",
    "    cross_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,        \n",
    "    num_workers=4,       \n",
    "    pin_memory=True      \n",
    ")\n",
    "\n",
    "# Longitudinal dataset (same person at different ages)\n",
    "# You'll need to structure this to return pairs\n",
    "\n",
    "longitudinal_pairs = get_longitudinal_sectional(\"/projects/standard/csci5561/shared/G8/data/AgeDB.json\")\n",
    "longitudinal_dataset = LongitudinalPairDataset(longitudinal_pairs)\n",
    "\n",
    "longitudinal_dataloader = DataLoader(\n",
    "        longitudinal_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=True,        \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"\\nModel architecture ready for training!\")\n",
    "\n",
    "# Full training loop example\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss_cross = 0\n",
    "    epoch_loss_long = 0\n",
    "    \n",
    "    # Train on cross-sectional data (shuffled each epoch)\n",
    "    for batch_idx, batch in enumerate(cross_dataloader):\n",
    "        loss = trainer.training_step_cross_sectional(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss_cross += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Cross-sectional batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Train on longitudinal data (shuffled each epoch)\n",
    "    for batch_idx, batch in enumerate(longitudinal_dataloader):\n",
    "        loss = trainer.training_step_longitudinal(\n",
    "            batch['img1'].to(device),\n",
    "            batch['age1'].to(device),\n",
    "            batch['img2'].to(device),\n",
    "            batch['age2'].to(device),\n",
    "            device\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss_long += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Longitudinal batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Print epoch summary\n",
    "    avg_loss_cross = epoch_loss_cross / len(cross_dataloader)\n",
    "    avg_loss_long = epoch_loss_long / len(longitudinal_dataloader)\n",
    "    print(f\"\\n=== Epoch {epoch} Complete ===\")\n",
    "    print(f\"Avg Cross-sectional Loss: {avg_loss_cross:.4f}\")\n",
    "    print(f\"Avg Longitudinal Loss: {avg_loss_long:.4f}\\n\")\n",
    "    \n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss_cross': avg_loss_cross,\n",
    "            'loss_long': avg_loss_long,\n",
    "        }, f'epochs_attempt1/checkpoint_epoch_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020578c-8c28-400c-8dee-a3e85effe330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
